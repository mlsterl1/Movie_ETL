{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challengemovie(wiki_movies, kaggle_data, ratings): \n",
    "    # load file \n",
    "    file_to_load = 'wikipedia.movies.json'\n",
    "    \n",
    "    # read file\n",
    "    with open('wikipedia.movies.json', mode='r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # Extract Kaggle data\n",
    "    kaggle_metadata = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "    ratings = pd.read_csv('ratings.csv')\n",
    "    \n",
    "    # Create list comprehension to filter wiki movies to include only movies with a director and IMDB link in movie and to filter out Tv shows \n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "                   if ('Director' in movie or 'Directed by' in movie)\n",
    "                       and 'imdb_link' in movie\n",
    "                       and 'No. of episodes' not in movie]\n",
    "    def clean_movie(movie):\n",
    "        movie = dict(movie) #create a non-destructive copy\n",
    "        alt_titles = {}\n",
    "        # combine alternate titles into one list\n",
    "        for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "            if key in movie:\n",
    "                alt_titles[key] = movie[key]\n",
    "                movie.pop(key)\n",
    "        if len(alt_titles) > 0:\n",
    "            movie['alt_titles'] = alt_titles\n",
    "\n",
    "        # merge column names\n",
    "        def change_column_name(old_name, new_name):\n",
    "            if old_name in movie:\n",
    "                movie[new_name] = movie.pop(old_name)\n",
    "        change_column_name('Adaptation by', 'Writer(s)')\n",
    "        change_column_name('Country of origin', 'Country')\n",
    "        change_column_name('Directed by', 'Director')\n",
    "        change_column_name('Distributed by', 'Distributor')\n",
    "        change_column_name('Edited by', 'Editor(s)')\n",
    "        change_column_name('Length', 'Running time')\n",
    "        change_column_name('Original release', 'Release date')\n",
    "        change_column_name('Music by', 'Composer(s)')\n",
    "        change_column_name('Produced by', 'Producer(s)')\n",
    "        change_column_name('Producer', 'Producer(s)')\n",
    "        change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "        change_column_name('Productioncompany ', 'Production company(s)')\n",
    "        change_column_name('Released', 'Release Date')\n",
    "        change_column_name('Release Date', 'Release date')\n",
    "        change_column_name('Screen story by', 'Writer(s)')\n",
    "        change_column_name('Screenplay by', 'Writer(s)')\n",
    "        change_column_name('Story by', 'Writer(s)')\n",
    "        change_column_name('Theme music composer', 'Composer(s)')\n",
    "        change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "        return movie\n",
    "    \n",
    "    # Create a list for clean movies using list comprehension\n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "    #set the data frame from wiki_movies_df to include the data in clean_movies\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "    \n",
    "    # extract IMDb ID\n",
    "    wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    \n",
    "    # Remove IMDb_ID duplicates\n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    \n",
    "    # List comprehensions clean up for columns to keep \n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    # Put wiki_columns_to_keep in the wiki_movies_df \n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    \n",
    "    # create variable for droped box offcie data \n",
    "    box_office = wiki_movies_df['Box office'].dropna() \n",
    "    \n",
    "    # Create a string seprator and then join it \n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # create from_one and use a regular expression to make data consistent \n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    \n",
    "    # create form_two and use a regular expression to make data consistent \n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    \n",
    "    # create variable that represent form 1 and 2 to find missing forms\n",
    "    matches_form_one = box_office.str.contains(form_one, flags=re.IGNORECASE)\n",
    "    matches_form_two = box_office.str.contains(form_two, flags=re.IGNORECASE)\n",
    "    \n",
    "    # find data that begins with $ and ends with hyphen and replace with $ only\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    # extract box_offcie using form_one and form_two \n",
    "    box_office.str.extract(f'({form_one}|{form_two})')\n",
    "    \n",
    "    def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "        # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "            # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "            # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "            # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "            # return value\n",
    "            return value\n",
    "\n",
    "        # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # drop box offcie column \n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # create variable for droped budget \n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    \n",
    "    # Create a string seprator and then join it \n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    \n",
    "    # create series to indetify which data does not match \n",
    "    matches_form_one = budget.str.contains(form_one, flags=re.IGNORECASE)\n",
    "    matches_form_two = budget.str.contains(form_two, flags=re.IGNORECASE)\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    \n",
    "    # remove citation references\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    budget[~matches_form_one & ~matches_form_two]\n",
    "    \n",
    "    # Parse Budget data \n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    \n",
    "    # drop budget column \n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    #create a varaible for release dates and convert to a string \n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    # Parse the forms\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    #Extract the release date \n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "\n",
    "    # Use built in infer_datetime function in Pandas and set to true \n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    \n",
    "    # create a vriable for running time and convert to string \n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "           \n",
    "    # extract digits only and allow for patterns in the data to be recognized \n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    \n",
    "    # convert string into numeric value \n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    \n",
    "    # convert hours and minutes to minutes if minutes group is 0\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    \n",
    "    # drop running time \n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    #keep row where adult data false and drop adult column \n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    \n",
    "    # create a  boolean column and assign it back  to video \n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    \n",
    "    # convert kaggle data to numeric values \n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    \n",
    "    #convert release date from standard to datetime format\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    \n",
    "    # assign rating to timestamp column \n",
    "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    \n",
    "    # Merge wiki_movies, Kaggle_metadata on IMDb ID.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "    # Drop outlieing movie \n",
    "    movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    \n",
    "    # drop title_wiki, release_date_wiki, language and production companies \n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    \n",
    "    # function to fill in missing data and drop columns \n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "        \n",
    "    # defines what missing data to fill in with zeros \n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    \n",
    "    # reorder the columns \n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                           'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                           'genres','original_language','overview','spoken_languages','Country',\n",
    "                           'production_companies','production_countries','Distributor',\n",
    "                           'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                          ]]\n",
    "    \n",
    "    #rename the columns \n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'budget_kaggle':'budget',\n",
    "                      'release_date_kaggle':'release_date',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Writer(s)':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "    \n",
    "    # group by rating and movie ID , rename userID to count , make movie ID the index\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "    \n",
    "    # prepend rating to each column \n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    #left merge\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    #fill in blanks with zero values \n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # connect sql \n",
    "    db_string = f\"postgres://postgres:{db_password}@localhost/movie_data\"\n",
    "    \n",
    "    # create database engine\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    try:\n",
    "        #delete rows from movies table \n",
    "        from sqlalchemy.orm import sessionmaker\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        session.execute('''TRUNCATE TABLE movies''')\n",
    "        session.commit()\n",
    "        session.close()\n",
    "    except:\n",
    "        print(\"deleting movie exception\")\n",
    "    try:\n",
    "        # delete rows from ratings table\n",
    "        from sqlalchemy.orm import sessionmaker\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "        session.execute('''TRUNCATE TABLE ratings''')\n",
    "        session.commit()\n",
    "        session.close()\n",
    "    except:\n",
    "        print(\"deleting ratings exception\")\n",
    "    \n",
    "    # save movie database to sql and append new data if table exists\n",
    "    movies_df.to_sql(name='movies', con=engine, if_exists='append')\n",
    "    \n",
    "    #import rating to sql using chunk size parameter \n",
    "    rows_imported = 0\n",
    "    # get the start_time from time.time()\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'ratings.csv', chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        rows_imported += len(data)\n",
    "\n",
    "        # add elapsed time to final print out\n",
    "        print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 139.66332578659058 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 278.0085554122925 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 417.8144636154175 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 562.6736030578613 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 711.1630573272705 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 853.6116347312927 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 988.5526757240295 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 1120.1439294815063 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 1252.0970404148102 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 1383.4827988147736 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 1638.500916004181 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 1764.9978594779968 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 1892.7120752334595 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 2025.6830606460571 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 2160.6442694664 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 2307.460898399353 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 2447.2868065834045 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 2582.721415042877 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 2715.417507171631 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 2847.879473924637 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 2989.6137821674347 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 3122.9196350574493 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 3255.928718328476 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 3391.998990058899 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 3526.2249789237976 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 3661.463760137558 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 3664.666756629944 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "challengemovie(\"wiki_movies.json\", \"movies_metadata.csv\", \"ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
